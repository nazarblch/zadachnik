\section{Элементы теории информации}

\begin{problem}
Пусть для некоторого пункта (скажем, для г. Долгопрудный) 
вероятность того, что $15$ июня будет дождь, равна $0.4$, а вероятность того, 
что дождя не будет, равна $0.6$. Пусть далее для этого же пункта вероятность 
дождя $15$ октября равна $0.8$, а вероятность отсутствия осадков равна $0.2$. 
Предположим, что определенный метод прогноза погоды $15$ июня оказывается 
верным в $3/5$ всех тех случаев, когда предсказывается дождь 
и в $4/5$ тех случаев, в которых прогнозируется отсутствие осадков. Применительно к 
погоде на $15$ октября этот метод оказывается правильным в $9/10$ тех случаев, 
когда предсказывается дождь, и в половине случаев, когда предсказывается его 
отсутствие. В какой из указанных двух дней прогноз дает нам больше 
информации о реальной погоде?
\end{problem}

\begin{problem}

Пусть казино делает $n$ бросаний, используя распределение вероятностей на бинарных словах длины $n$ $p\left(x\right)$, где $x=\left\{0,1\right\}^{n} $, известное игроку. При этом казино производит выплаты так, как если бы оно использовало распределении $q\left(x\right)$ (то есть выигранная ставка на 0, после выпадения $x$ исходов увеличивается в $\frac{q\left(x\right)}{q\left(x0\right)} $ раз, выигранная ставка на 1, после выпадения $x$ исходов увеличивается в $\frac{q\left(x\right)}{q\left(x1\right)} $ раз, см. семинар). Докажите, что у игрока есть стратегия, логарифм значения капитала которой равен расстоянию Кульбака-Лейблера $\sum _{x\in \left\{0,1\right\}^{n} }p\left(x\right)\log \frac{p\left(x\right)}{q\left(x\right)}  $ между распределениями $p$ и $q$.
\end{problem}


\subsection{Основные определения}
Пусть $X$ - дискретная случайная величина, принимающая значения из конечного множества (алфавита) $A = \{a_1,..., a_{|A|}\}$. $P = \{\mathbb{P}\{X = a_i\} = p_{a_i}\}$ - вероятностное распределение $X$ на $A$.

\begin{definition} \textit{Словом} в алфавите $A$ будем называть реализацию последовательности случайных величин $X_1,...,X_n..$: $w = (x_1...x_n..)$, $x_i \in A$.
\end{definition}

\begin{definition} 
\textit{Энтропией} $H(P)$ вероятностного распределения $P$ называется величина:
\begin{center}
$H(P) = - \sum_{a \in A} p_a\log p_a$, где $\log = \log_2$.
\end{center}
Энтропия измеряется в битах и интерпретируется как мера неопределенности или информационного содержания случайной величины. Чем она больше, тем больше неопределенность. В качестве иллюстрации, читателю предлагается решить первую задачу.
\end{definition}

\begin{definition}
\textit{Относительной энтропией} распределений $P$ и $Q$ на множестве $A$ (или расстоянием Кульбака-Лейблера между ними) называется
\begin{center}
$KL(P||Q) = \sum_{a \in A} p_a \log \frac{p_a}{q_a}$. 
\end{center}
В статистике эта величина определяет то, насколько "неэффективно" использование распределения $Q$ для аппроксимации распределедния $P$, или как много дополнительных бит мы заплатим за такую аппроксимацию.
\end{definition}

\subsection{Задачи}

\begin{problem} 
\begin{enumerate}
\item Ф.М. Достоевский решил изменить своим привычкам и отправился на скачки. У него есть предворительные (априорные) данные о том, какие шансы на победу имеет каждая из восьми лошадей-участниц: $(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$. Оцените энтропию, которая содержится в такой информации. 
\item Сравните результат со случаем, когда все исходы равновероятны. Какое из двух респределений содержит больше информации?
\item Докажите в общем случае, что из всех дискретных распределений на множестве $A$, наибольшей энтропией обладает равномерное.
\end{enumerate}
\end{problem}

\begin{problem}
Пусть $\{X_i\}_{i=1}^n$ - незавизимые в совокупности одинаково распределенные случайные величины с распределением $P = \{p_{a_i}\}$. Доказать, что 
\begin{equation}
-\frac{1}{n} \sum_{i = 1}^n \log P(X_i) \rightarrow^{\mathbb{P}} H(X_1)
\end{equation}
Или, другими словами, $\forall \delta, \epsilon > 0 \exists n_0$ такое что $\forall n \geq n_0$:
\begin{equation}
\mathbb{P}(|-\frac{1}{n} \sum_{i = 1}^n \log P(X_i) - H(X_1)| < \delta) > 1-\epsilon.
\end{equation}

\begin{ordre}
Воспользутесь тем, что $-\mathbb{E} \log P(X) = H(X)$.
\end{ordre}
\end{problem}

\begin{remark} При достаточно больших значениях $n$ можно определить множество \textit{типичных последовательностей} или \textit{слов}, энтропия которых близка к истинной энтропии распределения $P$. Вероятность появления слова $w$
\begin{equation}
p_w = p_{x_1}...p_{x_n} = 2^{-n (-\frac{1}{n} \sum_{i = 1}^n \log P(X_i))}.
\end{equation}
\textit{Множеством $\delta$-типичных $n$-буквенных слов} назовем $T_{\delta}^{(n)}$:
\begin{equation}
T_{\delta}^{(n)} = \{w: 2^{-n(H(X) + \delta)} < p_w < 2^{-n(H(X) - \delta)} \}
\end{equation}
\end{remark}

\begin{problem}
Доказать, что:
\begin{enumerate}
\item Множество типичных слов ограничено: $|T_{\delta}^{(n)}| \leq 2^{n(H(X) + \delta)}$;
\item $|T_{\delta}^{(n)}| \geq (1-\epsilon)2^{n(H(X) + \delta})$ для достаточно больших $n$;
\item вероятность того, что $w$ - нетипично: $\mathbb{P}\{w \neq T_{\delta}^{(n)} \} \leq \epsilon$.
\end{enumerate}
\end{problem}

\begin{remark} Идея о типичных последовательностях лежит в основе кодирования (эффективного сжатия данных). $\delta$-типичные $n$-буквенные слова кодируются при помощи двоичных последовательностей длины $n(H(X) + \delta)$, нетипичные отбрасываются или представляются одним и тем же добавочным символом. Очевидно, что при декодировании (восстановлении) вероятность ошибки не превысит $\epsilon$.
\end{remark}

\begin{problem}
\textit{Закон больших чисел}
$\forall \mathbb{P} \in \mathcal{P}_n $, $\forall \mathbb{Q}$ на $A$, $\forall \epsilon > 0$ типичным $T_{\mathbb{Q}}^{\epsilon}$ множеством слов для распределения $\mathbb{Q}^n$ будем называть
\begin{center}
$T_{\mathbb{Q}}^{\epsilon} = \{x_1^n: D(\mathbb{P}_{x_1^n}|| \mathbb{Q}) \leq \epsilon \}$.
\end{center}
Пусть $X_1, ..., X_n$ независимые одинаково распределенные с.в. ($X_i \sim \mathbb{Q}$). Показать, что:
\begin{enumerate}
\item 
\begin{center}
$Pr\{D(\mathbb{P}_{x_1^n} || \mathbb{Q}) \geq \epsilon \} \leq 2^{-n(\epsilon - |A|\frac{\log(n+1)}{n})}$
\end{center}
\textit{Указание}:
Пользуясь результатами задач 1.1 и 1.3 оценить вероятность того, что последовательность $x_1^n$ - нетипична: $Pr\{x_1^n \not\in T_{\mathbb{Q}}^{\epsilon}\} = 1 - \mathbb{Q}^n(T_{\mathbb{Q}}^{\epsilon})$;\\
для этого представить $1 - \mathbb{Q}^n(T_{\mathbb{Q}}^{\epsilon}) = \sum_{\mathbb{P}: D(\mathbb{P}||\mathbb{Q}) > \epsilon}{ \mathbb{Q}^n(T(\mathbb{P}))}$ 
\item $D(\mathbb{P}_{x_1^n}||\mathbb{Q})\rightarrow^{a.s.}0$\\
\textit{Указание}: воспользуйтесь первой леммой Борелля-Кантелли.\\
Пусть $\{A_n, n \geq \}$ - последовательность событий в вероятностном пространстве. $A = \cup_{k = 1}^{\infty} \cap_{n = k}^{\infty} A_n$ - событие, говорящее о том, что из серии $\{A_n, n \geq \}$ реализовалось бесконечно много событий.
Если $\sum_{n=1}^{\infty}\mathbb{P}(A_n) < \infty$, то $\mathbb{P}(A) = 0$
\end{enumerate}
\end{problem}

\begin{problem} \textit{Неравенство больших уклонений}
Рассмотрим последовательность незовисимых одинаково распределенных случайных величин $X_1, ..., X_n$, $X_i~Be(q)$. Доказать, что $-\frac{1}{n}\log \mathbb{P}(\frac{1}{n}\sum X_i \geq p) \rightarrow p\log \frac{p}{q} + (1-p)\log \frac{1-p}{1-q} = KL((p, 1-p)||(q, 1-q))$.
\begin{ordre}
\end{ordre}
\end{problem}

\begin{remark} 
Этот результат является следствием теоремы Санова:
\end{remark}

\begin{problem} \textit{Теорема Санова или неравенство больших уклонений.} Сделать указанием.\\
Пусть $X_1, ..., X_n$ независимые одинаково распределенные с.в. ($X_i \sim \mathbb{Q}$). $E$ - множество вероятностных распределений. Доказать, что
\begin{equation}
\mathbb{Q}^n(E) = \mathbb{Q}^n(E\cap \mathcal{P}_n) \leq (n+1)^{|A|}2^{-nD(\mathbb{P}^*||\mathbb{Q})},
\end{equation}
где $\mathbb{Q}^n(E) = \mathbb{Q}^n(E\cap \mathcal{P}_n)$ $\mathbb{P}^* = arg\min_{\mathbb{P}\in E}D(\mathbb{P}||\mathbb{Q})$ - ближайшее к $\mathbb{Q}$ в смысле расстояния Кульбака-Лейблера  распрделение из семейства $E$.\\
Если $E$ - замкнуто, то
\begin{equation}
\frac{1}{n} \log \mathbb{Q}^n(E) \rightarrow D(\mathbb{P}^*|| \mathbb{Q})
\end{equation}
\end{problem}



\begin{problem} 
\textit{Расстоянием по вариации} между двумя распределениями называется $||\mathbb{P}_1 - \mathbb{P}_2||_1 = \sum_{a \in A} |\mathbb{P}_1(a) - \mathbb{P}_2(a)|$. Доказать, что между ним и расстоянием Кульбака-Лейблера справедливо следующее соотношение:\\
$D(\mathbb{P}_1||\mathbb{P}_2) \geq \frac{1}{2\ln2} ||\mathbb{P}_1 - \mathbb{P}_2||_1^2$
\end{problem}