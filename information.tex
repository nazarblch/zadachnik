


\section{Элементы теории информации}
\begin{comment}
\subsection{Основные определения}
Пусть $X$ - дискретная случайная величина, принимающая значения из конечного множества (алфавита) $A = \{a_1,..., a_{|A|}\}$. $P = \{\mathbb{P}\{X = a_i\} = p_{a_i}\}$ - вероятностное распределение $X$ на $A$.

\begin{definition} \textit{Словом} в алфавите $A$ будем называть реализацию последовательности случайных величин $X_1,...,X_n..$: $w = (x_1...x_n..)$, $x_i \in A$.
\end{definition}

\begin{definition} 
\textit{Энтропией} $H(X)$ случайной величины $X$ распределенной по закону $P$ называется:
\begin{center}
$H(X) = - \sum_{a \in A} p_a\log p_a$, где $\log = \log_2$.
\end{center}
Иногда вместо $H(X)$  используется запись $H(P)$.
Энтропия измеряется в битах и интерпретируется как мера неопределенности или информационного содержания случайной величины. Чем она больше, тем больше неопределенность. В качестве иллюстрации, читателю предлагается решить первую задачу.
\end{definition}
В случае нескольких случайных величин можно определить два тесно связанных понятия: \textit{условной энтропии} и \textit{совместной информации}.
\begin{definition}
\textit{Условной энтропией} двух с.в. $X$ и $Y$ называется $H(X|Y) = \sum_{a \in A} \mathbb{P}(Y = a)H(X|Y=a)$, где $H(X|Y=a) = \sum_{a' \in A} \frac{\mathbb{P}(X = a', Y = a)}{\mathbb{P}(Y = a)} \log \frac{\mathbb{P}(X = a', Y = a)}{\mathbb{P}(Y = a)}$. Условная энтропия характеризует ту среднюю степень неопределнности, содержащейся в $X$, если имеется некоторая информация об $Y$.
\end{definition}

\begin{definition}
\textit{Совместная информация} $I(X,Y) = H(X) - H(X|Y)$ определяет то, сколько информации об $X$ содержится в $Y$.
\end{definition}

\begin{definition}
\textit{Относительной энтропией} случайных величин $X \backsim P$ и $Y \backsim Q$ на множестве $A$ (или расстоянием Кульбака-Лейблера между ними) называется
\begin{center}
$KL(P||Q) = \sum_{a \in A} p_a \log \frac{p_a}{q_a}$. 
\end{center}
В статистике эта величина определяет то, насколько "неэффективно" использование распределения $Q$ для аппроксимации распределедния $P$, или как много дополнительных бит мы заплатим за такую аппроксимацию.
\end{definition}

\begin{definition}
\textit{Кодом} слова $w \in A^{len(w)}$ в алфавите $\Sigma$ называется отображение $C(w) : w \rightarrow \sigma$, $\sigma \in B^{len(\sigma)}$. $len(\sigma)$ - длина кодового слова.
\end{definition}

\subsection{Задачи}
\end{comment}
\begin{problem} \textit{Цена информации.} Имеется неизвестное число от $1$ до $n$, $n \geq 2$. Разрешается задавать любые вопросы с ответами да/нет. При этом при ответе да игрок платит 1 рубль, а при ответе нет - 2 рубля. Сколько необходимо и достаточно запдатить для отгадывания числа?
\end{problem}

\begin{comment}

\begin{problem} \textit{Аксимоматиеское определение энтропии}.
Покажите, что приведенное выше определение энтропии естественным образом вытекает из следующих требований, накладываемых на величину, служащую количественной характеристикой меры неопределенности:
\begin{enumerate}
\item Значение функции $H(X)$ не меняется при перестановке чисел ${p_{a_1},..., p_{a_n}}$,
\item $H(X)$ непрерывная функция,
\item Выполняется равенство $H(p_1,...,p_n) = H(p_1 + p_2, p_3,..., p_n) + (p_1 + p_2) H(\frac{p_1}{p_1+p_2},\frac{p_2}{p_1+p_2} )$
\end{enumerate}
\end{problem}

\begin{problem} 
\begin{enumerate}
\item Ф.М. Достоевский решил изменить своим привычкам и отправился на скачки. У него есть предворительные (априорные) данные о том, какие шансы на победу имеет каждая из восьми лошадей-участниц: $(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$. Оцените энтропию, которая содержится в такой информации. 
\item Сравните результат со случаем, когда все исходы равновероятны. Какое из двух респределений содержит больше информации?
\item Докажите в общем случае, что из всех дискретных распределений на дискретном множестве $A$, наибольшей энтропией обладает равномерное.
\end{enumerate}
\end{problem}

\begin{problem}
В таблице приведен прогноз погоды в г. Долгопрудный:
$p$ - вероятность наличия/отсутствия осадков, $q$ - вероятсноть того, что прогноз окажется верным.
\begin{table}[h]
\caption{Прогноз погоды}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 &$p_{rain}$  &$p_{fine}$ & $q_{rain}$ & $q_{fine}$  \\
\hline
$15$ июня & $0.4$ & $0.6$ & $\frac{3}{5}$ & $\frac{4}{5}$\\
\hline
$15$ октября & $0.8$ & $0.2$ & $\frac{9}{10}$ & $\frac{1}{2}$\\
\hline
\end{tabular}
\end{center}
\end{table}
В какой из указанных двух дней прогноз дает нам больше информации о реальной погоде?
\begin{ordre}
\end{ordre}
\end{problem}

\end{comment}


\begin{problem}
Пусть $\{X_i\}_{i=1}^n$ - незавизимые в совокупности одинаково распределенные случайные величины с распределением $P = \{p_{a_i}\}$. Доказать, что 
\begin{equation}
-\frac{1}{n} \sum_{i = 1}^n \log P(X_i) \rightarrow^{\mathbb{P}} H(X_1)
\end{equation}
Или, другими словами, $\forall \delta, \epsilon > 0 \exists n_0$ такое что $\forall n \geq n_0$:
\begin{equation}
\mathbb{P}(|-\frac{1}{n} \sum_{i = 1}^n \log P(X_i) - H(X_1)| < \delta) > 1-\epsilon.
\end{equation}

\begin{ordre}
Воспользутесь тем, что $-\mathbb{E} \log P(X) = H(X)$.
\end{ordre}
\end{problem}

\begin{remark} При достаточно больших значениях $n$ можно определить множество \textit{типичных последовательностей} или \textit{слов}, энтропия которых близка к истинной энтропии распределения $P$. Вероятность появления слова $w$
\begin{equation}
p_w = p_{x_1}...p_{x_n} = 2^{-n (-\frac{1}{n} \sum_{i = 1}^n \log P(X_i))}.
\end{equation}
\textit{Множеством $\delta$-типичных $n$-буквенных слов} назовем $T_{\delta}^{(n)}$:
\begin{equation}
T_{\delta}^{(n)} = \{w: 2^{-n(H(X) + \delta)} < p_w < 2^{-n(H(X) - \delta)} \}
\end{equation}
\end{remark}

\begin{problem} \textit{Асимптотическая равнораспределенность.}
Доказать, что:
\begin{enumerate}
\item Множество типичных слов ограничено: $|T_{\delta}^{(n)}| \leq 2^{n(H(X) + \delta)}$;
\item $|T_{\delta}^{(n)}| \geq (1-\epsilon)2^{n(H(X) + \delta})$ для достаточно больших $n$;
\item вероятность того, что $w$ - нетипично: $\mathbb{P}\{w \neq T_{\delta}^{(n)} \} \leq \epsilon$.
\end{enumerate}
\end{problem}


\begin{comment}


\begin{remark} Идея о типичных последовательностях лежит в основе кодирования. Например, $\delta$-типичные $n$-буквенные слова кодируются при помощи двоичных последовательностей длины $n(H(X) + \delta)$, нетипичные отбрасываются или представляются одним и тем же добавочным символом. Очевидно, что при декодировании (восстановлении) вероятность ошибки не превысит $\epsilon$.
\end{remark}

\begin{problem} Рассмотрите связь между доказательством принципа асимптотической равнораспределенности и эквивалентностью (для больших систем)энтропий Больцмана и Гиббса.
\end{problem}


\end{comment}


\begin{problem} \textit{Задача о пропускной способности канала с шумом.} Канал связи с шумом описывается матрицей переходных вероятностей $p(y|x)$, $x \in A$, $y \in B$. $A$ - входной алфавит, $B$ - выходной алфавит. Другими словами, $p(y|x)$ - вероятность принять $y$ при условии, что был послан символ $x$. \textit{Пропускной способностью} такого канала называется величина $C = \max_{\{p_x\}} I(x, y)$. $\{p_x\}$ множество всех возможных распределений на входном алфавите $A$.
Рассмотрим двоичный симметричный канал, $A =\{0, 1\}$, $B =\{0, 1\}$. Каждая из букв передается без ошибок с вероятностями $p$ и $1-p$ соответственно (см.рис). Указать распределение, на котором достигается его максимальная пропускная способность.
\end{problem}

\begin{comment}

\begin{remark}
О связи совместная информации $I(x,y)$ и пропускной способности канала.  
\end{remark}

\begin{problem} \textit{Случайные коды.}
\end{problem}

\begin{problem} \textit{Оценка энтропии русского языка}
Рассмотрим алфавит $A$ состоящий из $34$х букв: $33$ буквы русского алфавита и пробел. На каждом шаге игроку необходимо угадать следующую букву текста, которая будет открыта, при условии, что он видит все буквы, открытые ранее. За каждую правильную догадку игрок получает $34$ рубля. Предложите стратегию, позволяющую оценить снизу энтропию русского языка.
\begin{ordre} Необходимо на каждом шаге выбирать ту букву, появление которой наиболее вероятно с учетом предыдущей информации. Тогда на шаге $n$ выигрыш может быть записан как $S_n = (34)^n \hat{p}(X_1...X_n)$, где $\hat{p}(X_1,...,X_n) = \sum_{a \in A} \hat{p}(a|x_{n-1}...x_1)$. Показать, что $\mathbb{E}\frac{1}{n}S_n \leq \log(34) + H(X)$, $H(X)$ - энтропия русского языка.
\end{ordre}   
\end{problem}
\begin{remark} Стоит ли писать что на этом подходе основывается построение кодов оптимальной длины?
\end{remark}

\begin{problem} 
\textit{Расстоянием по вариации} между двумя распределениями называется $||\mathbb{P}_1 - \mathbb{P}_2||_1 = \sum_{a \in A} |\mathbb{P}_1(a) - \mathbb{P}_2(a)|$. Доказать, что между ним и расстоянием Кульбака-Лейблера справедливо следующее соотношение:\\
$KL(\mathbb{P}_1||\mathbb{P}_2) \geq \frac{1}{2\ln2} ||\mathbb{P}_1 - \mathbb{P}_2||_1^2$
\end{problem}

\begin{problem}
Рассмотрим две монетки: правильную ($Be(p = \frac{1}{2})$) и неправильную ($Be(q = \frac{1 +\epsilon}{2})$). Оценить минимальное число бросаний, необходимое для того, чтобы отличить первую от второй.
\begin{ordre}
Получите оценку для $KL(p|q) \leq \frac{2\epsilon^2}{\ln 2}$.
\end{ordre}
\end{problem}

\begin{remark} Необходимость построения нижних оценок возникает тогда, когда случайная последовательность генерируется одним из распределений $\{P_i\}$ (неизвестно каким), а наилучшая стратегия игрока зависит от вида истинного $P_k \in \{P_i\}$, о котором нет никакой информации  . 
\end{remark}

\begin{problem}
Пусть казино делает $n$ бросаний, используя распределение вероятностей на бинарных словах длины $n$ $p\left(x\right)$, где $x=\left\{0,1\right\}^{n} $, известное игроку. При этом казино производит выплаты так, как если бы оно использовало распределении $q\left(x\right)$ (то есть выигранная ставка на 0, после выпадения $x$ исходов увеличивается в $\frac{q\left(x\right)}{q\left(x0\right)} $ раз, выигранная ставка на 1, после выпадения $x$ исходов увеличивается в $\frac{q\left(x\right)}{q\left(x1\right)} $ раз, см. семинар). Докажите, что у игрока есть стратегия, логарифм значения капитала которой равен расстоянию Кульбака-Лейблера $\sum _{x\in \left\{0,1\right\}^{n} }p\left(x\right)\log \frac{p\left(x\right)}{q\left(x\right)}  $ между распределениями $p$ и $q$.
\end{problem}


\begin{problem} \textit{Неравенство больших уклонений}
Рассмотрим последовательность незовисимых одинаково распределенных случайных величин $X_1, ..., X_n$, $X_i~Be(q)$. Доказать, что $-\frac{1}{n}\log \mathbb{P}(\frac{1}{n}\sum X_i \geq p) \rightarrow p\log \frac{p}{q} + (1-p)\log \frac{1-p}{1-q} = KL((p, 1-p)||(q, 1-q))$.
\begin{ordre}
\end{ordre}
\end{problem}

\begin{remark} 
Этот результат является следствием теоремы Санова, которая позволяет оценивать вероятности больших уклонений.. Приведем ниже её упрощенную формулировку.\\
\textit{Теорема Санова}
\end{remark}

\end{comment}