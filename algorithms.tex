\section{Вероятностные методы в Computer Science}

\subsection{Рандомизированные алгоритмы}

\begin{problem}

Требуется определить начиная с какого этажа брошенный с балкона 100-этажного здания стеклянный шар разбивается. В наличии имеется два таких шара. Предложить метод нахождения граничного этажа, минимизирующий математическое ожидание числа бросков. Рассмотреть случай большего числа шаров.  

\end{problem}


\begin{problem}
Имеется неизвестное число от 1 до $n$ ($n\ge 2$). Разрешается задавать любые вопросы с ответами ДА/НЕТ. При этом при ответе ДА мы платим 1 рубль, при ответе НЕТ -- 2 рубля. Сколько необходимо и достаточно заплатить для отгадывания числа?
\end{problem}

\begin{problem}
При нахождении скрытых величин генератора методом максимизации правдоподобия решается следующая оптимизационная задача.

\[
\underset{C}{\max} \; P(C \vert X) \varpropto P(X \vert C),
\]

\noindent где $X$ - множество известных сгенерированных случайных величин, $C$ - множество скрытых  сгенерированных величин.

Генератор представляет собой набор отношений (связей) между величинами из множеств $X$ и $C$. 
Каждая связь является распределением, участвующем в процессе генерации. 

Для решения указанной оптимизационной задачи предлагается подобрать распределение $q(C \vert \lambda)$ схожее с $P(C \vert X)$ по типам зависимостей. Далее, минимизируя расстояние Кульбака-Лейблера $\sum q\left(x\right)\log \frac{q\left(x\right)}{P\left(x\right)}  $ между распределениями $P$ и $q$, найти значения вектора параметров $\lambda$. Вычислив $\lambda$, можно найти сами скрытые переменные $C$ ввиду простой структуры распределения $q(C \vert \lambda)$, которое обычно задается в виде:

\[
q(C \vert \lambda) = \prod q_i(C_i \vert \lambda_i) 
\]        

Рассмотрим пример конкретный пример генератора.

\[
\theta_d \sim Dirichlet(\alpha) \; d = \overline{1,m}
\]
\[
\phi_t \sim Dirichlet(\beta) \; t = \overline{1,K}
\]
\[
z_{di} \sim Categorical(\theta_d) \; i = \overline{1,n_d}
\]
\[
w_{di} \sim Categorical(\phi_{z_{di}}) \; i = \overline{1,n_d},
\]
 
где $W = X$, $C = (\theta, \phi, Z)$, 

$\alpha, \beta, m, n_d, K$ - известные параметры генератора.  

Пусть 
\[
q(\theta \phi, Z \vert \lambda) = \underset{d}{\prod}\underset{t}{\prod}\underset{i}{\prod} Dir(\theta_d | \lambda_{d}) Dir(\phi_t | \eta_{t}) Cat(z_{di} | \lambda_{di})
\]

Требуется найти конечный вид системы уравнений 

\[
\frac{\partial \sum q\left(x\right)\log \frac{q\left(x\right)}{P\left(x\right)} } {\partial \lambda} = 0
\]  

для данного генератора.

\end{problem}

\begin{problem}
Пять философов сидят за круглым столом. В центре стола находится чаша со
спагетти. Между каждой парой соседних философов лежит вилка. Философы чередуют размышления с приемами пищи. Каждый философ может либо есть, либо размышлять. Приём пищи не ограничен
количеством оставшихся спагетти — подразумевается бесконечный запас. Однако
для того, чтобы вытащить спагетти из чаши и донести их до рта философу требуются
две вилки. Таким образом, философ может есть только взяв вилки слева и справа от
себя. 

Каждый философ может взять вилку рядом с ним (если она доступна), или положить
- если он уже держит её. Взятие каждой вилки и возвращение её на стол являются
раздельными действиями, которые должны выполняться одно за другим. Если
требуемая вилка занята соседом, голодный философ вынужден ждать - он не
может вернуться к размышлениям, не поев. После окончания еды философ кладет
обе вилки на стол для того, чтобы ими могли воспользоваться другие философы.

Время одного приема пищи одним философом равномерно распределено на отрезке [0, a]. 
Время одного размышления равномерно распределено на отрезке [0, b].

Данный процесс подвержен взаимной блокировке (Dead Lock): например, если каждый возьмет по левой вилке, то начнется вечное голодание. Для избежания блокировки каждый философ ложит первую вилку, если за время t после ее взятия вторая не освободилась.

Требуется определить распределение времени t, минимизирующее среднее время ожидания после размышления и перед приемом пищи.

\end{problem}


\begin{problem}
\noindent Алгоритм быстрой сортировки основан на парадигме «разделяй и властвуй». Выбирается из элементов массива опорный элемент, относительно которого переупорядочиваются все остальные элементы. Желательно выбрать опорный элемент близким к значению медианы, чтобы он разбивал список на две примерно равные части. Переупорядочивание элементов относительно опорного, происходит так, что все переставленные элементы, лежащие левее опорного, меньше его, а те, что правее -- больше или равны опорному. Далее процедура быстрой сортировки рекурсивно применяется к левому и правому списку для их упорядочивания по отдельности.

Наихудшие входные данные для описанного алгоритма быстрой сортировки (предполагается, что в качестве опорного элемента выбирается последний элемент обрабатываемого массива) -- элементы уже упорядоченные по возрастанию. 
Откуда следует, что асимптотика времени работы быстрой сортировки в худшем случае $\Theta (n^{2} )$.

Оценить время работы алгоритма быстрой сортировки в среднем. 

\begin{ordre}
Получить рекуррентное соотношение для математического ожидания времени работы, введя индикаторную функцию позиции опорного элемента. 
Воспользоваться соотношением:
\[\begin{array}{l} {\sum _{k=1}^{n-1}k\log k \le \log \frac{n}{2} \sum _{k=1}^{\left\lceil \frac{n}{2} \right\rceil -1}k +\log n\sum _{k=\left\lceil \frac{n}{2} \right\rceil }^{n-1}k =} \\ {=\frac{n(n-1)}{2} \log n-\frac{\left\lceil \frac{n}{2} \right\rceil \left(\left\lceil \frac{n}{2} \right\rceil -1\right)}{2} \le \frac{1}{2} n^{2} \log n-\frac{n^{2} }{8} } \end{array}\] 
 
\end{ordre}

Показать неулучшаемость оценки для произвольного алгоритма сортировки. Привести способ сортировки с асимптотикой $O(n \log n)$ в худшем случае.

\end{problem}

\begin{problem}

(Задача поиска k-ой порядковой статистики).
Рекурсивное применение процедуры, основанной на методе быстрой сортировки, позволяет быстро (в среднем) находить k-ую порядковую статистику. Задача вычисления порядковых статистик состоит в следующем: дан список (массив) из $n$ чисел, необходимо найти значение, которое стоит в k-ой позиции в отсортированном в возрастающем порядке списке. 

\noindent Модифицируем алгоритм быстрой сортировки:


 Выбираем опорный элемент. Делим список на две группы. В первой -- элементы меньше опорного, во второй -- больше либо равны.
 Если размер (число элементов) первой группы больше либо равен k, то к ней снова применяется эта процедура. Иначе -- вызывается процедура для второй группы.
 
\noindent Покажите, используя ту же технику, что и при анализе в среднем алгоритма быстрой сортировки, что среднее время работы такого алгоритма линейно.

\begin{ordre}

\noindent Покажите, что выполняется оценка среднего времени работы алгоритма:

\[
E[T(n)]\le E\left\{\sum _{k=1}^{n}X_{k} \left[T\left(\max (k-1,n-k)\right)+O(n)\right] \right\} 
\]
\[ \le \frac{2}{n} \sum _{k=\left\lfloor \frac{n}{2} \right\rfloor }^{n-1}E[T(k)] +O(n)
\]



\end{ordre}

\begin{remark}

Пусть $a\ge 1$ и $b>1$- константы, $f(n)$ - произвольная функция, $T(n)$ - функция, определенная на множестве неотрицательных целых чисел с помощью рекуррентного соотношения: $T(n)=aT\left(\frac{n}{b} \right)+f(n)$

\noindent где выражение $\frac{n}{b} $интерпретируется либо как $\left\lfloor \frac{n}{b} \right\rfloor $, либо как$\left\lceil \frac{n}{b} \right\rceil $. Тогда асимптотическое поведение функции $T(n)$ можно выразить следующим образом. 

\begin{enumerate}
\item  Если $f(n)=O(n^{\log _{b} a-\varepsilon } )$ для некоторой константы $\varepsilon >0$, то $T(n)=\Theta \left(n^{\log _{b} a} \right)$.

\item  Если $f(n)=\Theta \left(n^{\log _{b} a} \right)$, то $T(n)=\Theta \left(n^{\log _{a} b} \lg n\right)$.

\item  Если $f(n)=\Omega (n^{\log _{b} a+\varepsilon } )$ для некоторой константы $\varepsilon >0$, и для некоторой константы $c<1$ и достаточно больших n выполнено: $af\left(\frac{n}{b} \right)\le cf(n)$, то $T(n)=\Theta \left(f(n)\right)$.
\end{enumerate}

\end{remark}
\end{problem}



\begin{problem}

Даны три матрицы $A,B,C$размера $n\times n$. Требуется проверить равенство $AB=C$.

Простой детерминированный алгоритм перемножает матрицы $A$, $B$ и сравнивает результат с $C$. Время работы такого алгоритма при использовании обычного перемножения матриц составляет $O(n^{3} )$, при использовании быстрого - $O(n^{2,376} )$. Вероятностный алгоритм Фрейвалда с односторонней ошибкой проверяет равенство за время $O(n^{2} )$.

Описание вероятностного алгоритма:

\begin{enumerate}
\item \textbf{ }взять случайный вектор $x\in \left\{0,1\right\}^{n} $

\item  вычислить $y=Bx$

\item  вычислить $z=Ay$

\item  вычислить $t=Cx$

\item  если $z=t$ вернуть «да», иначе «нет».
\end{enumerate}

\textbf{Лемма.} Покажем, что для предъявленного алгоритма выполняется 
\[\begin{array}{l} {P\left\{"40"|AB=C\right\}=1,} \\ {P\left\{"=5B"|AB\ne C\right\}\ge {\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/ {\vphantom {1 2}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ 2 $}} .} \end{array}\] 
То есть вероятность ошибки не более ${\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/ {\vphantom {1 2}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ 2 $}} $, при чем алгоритм ошибается только в случае $AB\ne C$.

\textbf{Доказательство.}

Если $AB=C$, то из описания алгоритм вернет «да».

Рассмотрим случай $AB\ne C$.

Алгоритм ошибается, если для $x$:
\[\left(AB-C\right)x=0.\] 

При этом матрица $AB-C$ ненулевая. 

Обозначим ее за матрицу $D$. Пусть $d=\left(d_{1} ,...,d_{n} \right)$ - ненулевая строка матрицы $D$. Без ограничения общности можно считать, что $d_{1} \ne 0$.
\[P\left\{Dx=0\right\}\le P\left\{d^{T} x=0\right\}=P\left\{x_{1} =-\frac{\sum _{i=2}^{n}d_{i} x_{i}  }{d_{1} } \right\}.\] 

Последнее равно ${\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/ {\vphantom {1 2}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ 2 $}} $, если $-\frac{\sum _{i=2}^{n}d_{i} x_{i}  }{d_{1} } \in \left\{0,1\right\}$, иначе равно $0$.

Лемма доказана.

\end{problem}

\begin{problem}

Требуется сравнить две битовые строки $a,b$, затратив как можно меньше информации. Основная идея -- сравнивать не сами строки, а функции от них. Так сравниваются $a\; mod\; p$ и $b\; mod\; p$, для некоторого простого числа $p$. Для этого требуется передать $2\log _{2} p$ бит информации.

Описание алгоритма сравнения строк:

\begin{enumerate}
\item  Пусть $\left|a\right|=\left|b\right|=n$, $N=n^{2} \log _{2} n^{2} $

\item  Выбираем случайное простое число $p$из интервала $\left[2..N\right]$ 

\item  Выдать «да», если $a\; mod\; p=b\; mod\; p\Leftrightarrow (a-b)\equiv 0\; mod\; p$, иначе выдать «нет».
\end{enumerate}

\noindent \textbf{Лемма}. Этот алгоритм является вероятностным алгоритмом с односторонней ошибкой

\noindent и вероятностью ошибки $O\left({\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/ {\vphantom {1 n}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ n $}} \right)$.То есть справедливо, что
\[\begin{array}{l} {P\left\{"40"\Leftrightarrow (a-b)\equiv 0,mod(p)|a=b\right\}=1,} \\ {P\left\{"40"\Leftrightarrow (a-b)\equiv 0,mod(p)|a\ne b\right\}=O\left({\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/ {\vphantom {1 n}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ n $}} \right),} \end{array}\] 
 При этом необходимое количество переданных бит равно $O\left(\log _{2} n\right)$.

\textbf{Доказательство}.

В основе доказательства лежит асимптотический закон распределения простых чисел:
\[\mathop{\lim }\limits_{n\to \infty } \frac{\pi \left(n\right)}{{\raise0.7ex\hbox{$ n $}\!\mathord{\left/ {\vphantom {n \ln n}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ \ln n $}} } =1,\] 
где $\pi \left(n\right)$ - функция распределения простых чисел, равная количеству простых чисел, не превосходящих $n$.

Алгоритм может ошибаться, когда $a\ne b$. Число таких простых чисел $p$, при которых алгоритм ошибочно выдает $"40"\Leftrightarrow (a-b)\equiv 0\; mod\; p$не превосходит $n$. 

Поэтому $P\left\{"40"\Leftrightarrow (a-b)\equiv 0\; mod\; p|a\ne b\right\}\le \frac{n}{{\raise0.7ex\hbox{$ N $}\!\mathord{\left/ {\vphantom {N \ln N}} \right. \kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ \ln N $}} } =\frac{n\left(\ln n^{2} +\ln \log _{2} n^{2} \right)}{n^{2} \ln n^{2} } =O\left(\frac{1}{n} \right)$.

При этом число передаваемых битов оценивается, как
\[2\log _{2} p\le 2\log _{2} N=2\log _{2} \left(n^{2} \log _{2} n^{2} \right)=O\left(\log _{2} n\right)\] 

\end{problem}


\begin{problem}

(Проверка простоты числа).

Согласно малой теореме Ферма, если $N$ - простое число и целое $a$ не делится на $N$, то  
\[a^{N-1} \equiv 1\; mod\; N                                     \left(*\right)\] 

Отсюда следует, что если при каком-то $a$ сравнение $\left(*\right)$ нарушается, то можно утверждать, что $N$ - составное. Вопрос только в том, как найти для составного $N$ целое $a$, не удовлетворяющее $\left(*\right)$. Можно, например, пытаться найти необходимое число $a$, испытывая все целые числа, начиная с 2. Или попробовать выбирать эти числа случайным образом на отрезке $1<a<N$.

К сожалению, такой подход не всегда дает то, что хотелось бы. Имеются составные числа $N$, обладающие свойством $\left(*\right)$ для любого целого $a$ с условием $\left(a,N\right)=1$ ($a$ и $N$ - взаимно простые). Такие числа называются числами Кармайкла.

Рассмотрим, например число $561=3\times 11\times 17$. Так как $560$ делится на каждое из чисел $2,\; 10,\; 16$, то с помощью малой теоремы Ферма легко проверить, что $561$ есть число Кармайкла. Недавно была решена проблема о бесконечности множества таких чисел.

В 1976г. Миллер предложил заменить проверку $\left(*\right)$ проверкой несколько иного условия. Если $N$ - простое число, то $N-1=2^{s} t$, где $t$ нечетно, то согласно малой теоремы Ферма для каждого  с условием $\left(a,N\right)=1$ хотя бы одна из скобок в произведении 
\[\left(a^{t} -1\right)\left(a^{t} +1\right)\left(a^{2t} +1\right)\times \ldots \times \left(a^{2^{s-1} t} +1\right)=a^{N-1} -1\] 
делится на $N$. Обращение этого свойства можно использовать, чтобы отличать составные числа от простых.

Пусть $N$ - нечетное составное число, $N-1=2^{s} t$, где \textbf{$t$ }нечетно. Назовем целое число $a$, $1<a<N$ «хорошим» для $N$, если нарушается одно из двух условий:

I) $N$ не делится на $a$

II) $a^{t} \equiv 1\; mod\; N$ или существует целое $k$, $0\le k<s$ такое, что $a^{2^{k} t} \equiv -1\; mod\; N$.



Ясно, что для простого числа \textbf{$N$ }не существует «хороших» чисел $a$. Если же $N$ составное число, то согласно теореме Рабина (ссылка??) их существует не менее $\frac{3}{4} \left(N-1\right)$.

Теперь можно поострить вероятностный алгоритм, отличающий составные числа от простых.

\textbf{Описание алгоритма проверки простоты числа:}

\begin{enumerate}
\item \textbf{ }Выберем случайным образом число $a$, $1<a<N$ и проверим для этого числа указанные выше свойства I и II.

\item  Если хотя бы одно из них нарушается, то число $N$ составное.

\item  Если выполнены оба условия I и II, возвращаемся к шагу 1)
\end{enumerate}

\noindent Из сказанного выше следует, что составное число не будет определено как составное после однократного выполнения шагов 1)-3) с вероятность не большей $\frac{1}{4} $. А вероятность не определить его после $k$ повторений не превосходит $\left(\frac{1}{4} \right)^{k} $, т.е. убывает очень быстро.


\end{problem}

\begin{problem}

$f(x_{1} ,...,x_{n} )=C_{1} \vee \cdots \vee C_{m} $ - булева формула в дизъюнктивной нормальной форме (ДНФ), где каждая скобка $C_{i} $ - есть конъюнкция $L_{1} \wedge \cdots \wedge L_{k_{i} } $ $k_{i} $ литералов (литерал есть либо переменная, либо ее отрицание). Набор значений переменных $a=(a_{1} ,...,a_{n} )$ называется выполняющим для $f$, если $f(a_{1} ,...,a_{n} )=1$. Найти число выполняющих наборов для данной ДНФ.

\noindent 

\noindent Рассмотрим алгоритм, основанный на стандартном методе Монте-Карло.

\noindent Пусть 

\noindent $V$ - множество всех двоичных наборов длины $n$.

\noindent $G$ - множество выполняющих наборов.

\begin{enumerate}
\item  Проведем $N$ независимых испытаний:
\end{enumerate}

\noindent Выбираем случайно $v_{i} \in V$ ( в соответствии с равномерным распределением)

\noindent $y_{i} =f(v_{i} )$. Заметим, что $P\left\{y_{i} =1\right\}=\frac{\left|G\right|}{\left|V\right|} =p$

\begin{enumerate}
\item  Рассмотрим сумму независимых случайных величин $Y=\sum _{i=1}^{N}y_{i}  $. В качестве аппроксимации $\left|G\right|$ возьмем величину $\frac{Y}{N} \left|V\right|$.
\end{enumerate}

\noindent Для дальнейшего анализ потребуется следующая лемма:

\noindent Пусть $X_{1} ,...,X_{n} $ - независимые случайные величины, принимающие значения 0 или 1, при этом $P\left\{X_{i} =1\right\}=p,\quad P\left\{X_{i} =0\right\}=1-p$. Тогда для $X=\sum _{i=1}^{N}X_{i}  $ и для любого $0<\delta <1$, выполнены неравенства
\[\begin{array}{l} {P\left\{X>(1+\delta )EX\right\}\le e^{-\frac{\delta ^{2} }{3} EX} } \\ {P\left\{X<(1-\delta )EX\right\}\le e^{-\frac{\delta ^{2} }{2} EX} } \end{array}\] 
Воспользовавшись этим утверждением, оценим качество аппроксимации решения в приведенном алгоритме Монте-Карло.
\[P\left\{(1-\delta )\frac{\left|G\right|}{\left|V\right|} N\le Y\le (1+\delta )\frac{\left|G\right|}{\left|V\right|} N\right\}>1-2e^{-\frac{\delta ^{2} }{3} N\frac{\left|G\right|}{\left|V\right|} } \] 
Потребуем, чтобы вероятность хорошей аппроксимации была не меньше $1-\varepsilon $, получим
\[\begin{array}{l} {2e^{-\frac{\delta ^{2} }{3} N\frac{\left|G\right|}{\left|V\right|} } <\varepsilon } \\ {N>\frac{3}{\delta ^{2} } \frac{\left|V\right|}{\left|G\right|} \ln \frac{2}{\varepsilon } } \end{array}\] 
Правда, полученная оценка не столь хороша, так как $p=\frac{\left|G\right|}{\left|V\right|} $ может быть экспоненциально мало (например, если функция равна 1 лишь в одной точке), и тогда число требующихся шагов будет экспоненциально велико. Можно модифицировать алгоритм, чтобы он работал полиномиальное число шагов.

\end{problem}


\begin{problem}

 Даны $m$ скобок конъюнктивной нормальной формы (КНФ) с $n$ переменными. Найти значения переменных, максимизирующее число выполненных скобок.

\noindent  

\noindent Следующее утверждение по существу дает приближенный вероятностный алгоритм решения.

 Для любых $m$ скобок существуют значения переменных, при которых выполнено не менее $\frac{m}{2} $ скобок.

\noindent Предположим, что каждой переменной приписаны значения 0 или 1 независимо и равновероятно. Для $1\le i\le m$ пусть $Z_{i} =1$, если i-ая скобка выполнена, и $Z_{i} =0$ в противном случае.

\noindent Для каждой дизъюнкции (скобки) с $k$ литералами (переменными или их отрицаниями) вероятность, что эта дизъюнкция не равна 1 при случайном приписывании значений переменных, равна $2^{-k} $. Значит вероятность того, что скобка рана 1, есть $1-2^{-k} \ge \frac{1}{2} $ и математическое ожидание $EZ_{i} \ge \frac{1}{2} $. Отсюда математическое ожидание числа выполненных скобок (равных 1) равно $\sum _{i=1}^{m}EZ_{i}  \ge \frac{m}{2} $. Это означает, что есть приписывание значений переменным, при котором $\sum _{i=1}^{m}Z_{i}  \ge \frac{m}{2} $

 Говорят, что вероятностный приближенный алгоритм гарантирует точность $C$, если для всех входов $I$
\[\frac{Em_{A} (I)}{m_{0} (I)} \ge C,\] 
где $m_{0} (I)$ - оптимум, $m_{A} (I)$ - значение, найденное алгоритмом, и решается задача максимизации. 

\noindent 

 Таким образом, описанный приближенный вероятностный алгоритм для задачи максимальной выполнимости дает точность $\frac{1}{2} $.

\noindent 

\noindent Можно подойти к этой задаче (максимальная выполнимость) по-другому, переформулировав ее в задачу целочисленного линейного программирования (ЦЛП). Каждой скобке $C_{j} $ поставим в соответствие булеву переменную $z_{j} \in \left\{0,1\right\}$, которая равна 1, если скобка $C_{j} $ выполнена; каждой входной переменной $x_{i} $ сопоставляем переменную $y_{i} $, которая равна 1, если $x_{i} =1$, и равна 0 в противном случае. Обозначим $C_{j}^{+} $ индексы переменных в скобке $C_{j} $, которые входят в нее без отрицания, а через $C_{j}^{-} $ - множество индексов переменных, которые входят в скобку с отрицанием. .Тогда задача максимальной выполнимости эквивалентна следующей задача ЦЛП:
\[\begin{array}{l} {\sum _{j=1}^{m}z_{j}  \to \max } \\ {\sum _{i\in C_{j}^{+} }y_{i} +\sum _{i\in C_{j}^{+} }(1-y_{i} )  \ge z_{j} \quad j=1..m} \\ {y_{i} ,\quad z_{j} \in \left\{0,1\right\}\quad \forall i=1..n,j=1..m} \end{array}\] 
Рассмотрим и решим задачу линейной релаксации целочисленной программы:
\[\begin{array}{l} {\sum _{j=1}^{m}\hat{z}_{j}  \to \max } \\ {\sum _{i\in C_{j}^{+} }\hat{y}_{i} +\sum _{i\in C_{j}^{+} }(1-\hat{y}_{i} )  \ge \hat{z}_{j} \quad j=1..m} \\ {\hat{y}_{i} ,\quad \hat{z}_{j} \in \left[0,1\right]\quad \forall i=1..n,j=1..m} \end{array}\] 


\noindent Пусть $\hat{y}_{i} ,\quad \hat{z}_{j} $ - решение  линейной релаксации. Ясно, что $\sum _{j=1}^{m}\hat{z}_{j}  $ является верхней оценкой числа выполненных скобок для данной КНФ.

\noindent Рассмотрим вероятностный алгоритм решения задачи максимальной выполнимости, где каждая переменная $y_{i} $ независимо принимает значения 0 или 1 уже не с равными вероятностями, а с вероятностью $\hat{y}_{i} $ принимает значение 1 (и 0 с вероятностью $1-\hat{y}_{i} $). Такой метод называется вероятностным округлением.

\noindent 

\noindent Докажем следующее утверждение:

\noindent Пусть в скобке $C_{j} $ имеется $k$ литералов. Вероятность того, что она выполнена при вероятностном округлении, не менее $\left(1-\left(1-\frac{1}{k} \right)^{k} \right)\hat{z}_{j} $.

\noindent Доказательство. Без ограничения общности можно предположить, что все переменные в скобке входят в нее без отрицания. Пусть эта скобка имеет вид: $x_{1} \vee \cdots \vee x_{k} $. Из ограничений линейной релаксации следует, что
\[\hat{y}_{1} +\cdots +\hat{y}_{k} \ge \hat{z}_{j} \] 
Скобка $C_{j} $ остается невыполненной, только если каждая из переменных $\hat{y}_{i} $ округляется в 0. Поскольку каждая переменная округляется независимо, это происходит с вероятностью $\prod _{i=1}^{k}(1-\hat{y}_{i} ) $. Остается показать, что
\[1-\prod _{i=1}^{k}(1-\hat{y}_{i} ) \ge \left(1-\left(1-\frac{1}{k} \right)^{k} \right)\hat{z}_{j} \] 
Выражение в левой части достигает минимума при $\hat{y}_{1} =...=\hat{y}_{k} =\frac{\hat{z}_{j} }{k} $. Остается показать, что $1-(1-z)^{k} \ge \left(1-\left(1-\frac{1}{k} \right)^{k} \right)z$ для всех положительных целых $k$ и $0\le z\le 1$.

\noindent Поскольку $f(x)=1-\left(1-\frac{x}{k} \right)^{k} $ - вогнутая функция, для доказательства того, что она не меньше линейной функции на отрезке, достаточно проверить это нестрогое неравенство на концах этого отрезка, т.е. в точках $x=0$ и $x=1$.

\noindent Использую тот факт, что $1-\left(1-\frac{1}{k} \right)^{k} \ge 1-\frac{1}{e} $ для всех положительных целых $k$, получаем, что справедлива следующая 

\noindent \textbf{Теорема} Для произвольной КНФ среднее число скобок, выполненное при вероятностном округлении, не меньше $1-\frac{1}{e} $ от максимально возможного числа выполненных скобок.

\noindent Теперь опишем общую идею, которая позволит получить вероятностный приближенный алгоритм с точностью $\frac{3}{4} $.

\noindent Идея: на данном входе запускаем два алгоритма и выбираем лучшее из решений. В качестве двух алгоритмов рассматриваем:

\noindent 1) округление каждой переменной  независимо в 0 или 1 с вероятностью $\frac{1}{2} $;

\noindent 2) вероятностное округление решения линейной релаксации соответствующей задачи ЦЛП.

\noindent 

\noindent Пусть$n_{1} $ - математическое ожидание числа выполненных скобок для первого алгоритма, и $n_{2} $ - математическое ожидание числа выполненных скобок для второго алгоритма.

\noindent \textbf{Теорема.}
\[\max \left\{n_{1} ,n_{2} \right\}\ge \frac{3}{4} \sum _{j}\hat{z}_{j}  \] 
Доказательство. Поскольку всегда $\max \left\{n_{1} ,n_{2} \right\}\ge \frac{n_{1} +n_{2} }{2} $, достаточно показать, что $\frac{n_{1} +n_{2} }{2} \ge \frac{3}{4} \sum _{j}\hat{z}_{j}  $. Пусть $S_{k} $ обозначает множество скобок, содержащих ровно $k$ литералов, тогда
\[n_{1} =\sum _{k}\sum _{C_{j} \in S_{k} }(1-2^{-k} )  \ge \sum _{k}\sum _{C_{j} \in S_{k} }(1-2^{-k} )\hat{z}_{j}   \] 
Из предыдущей теоремы
\[n_{2} \ge \sum _{k}\sum _{C_{j} \in S_{k} }\left(1-\left(1-\frac{1}{k} \right)^{k} \right)\hat{z}_{j}   \] 
Следовательно,
\[\frac{n_{1} +n_{2} }{2} \ge \sum _{k}\sum _{C_{j} \in S_{k} }\frac{\left(1-2^{-k} \right)+\left(1-\left(1-\frac{1}{k} \right)^{k} \right)}{2} \hat{z}_{j}   \] 
Простое вычисление показывает, что $\left(1-2^{-k} \right)+\left(1-\left(1-\frac{1}{k} \right)^{k} \right)\ge \frac{3}{2} $ для всех натуральных $k$ и, значит,
\[\frac{n_{1} +n_{2} }{2} \ge \frac{3}{4} \sum _{k}\sum _{C_{j} \in S_{k} }\hat{z}_{j}   =\frac{3}{4} \sum _{j}\hat{z}_{j}  \] 


\noindent Оказывается, в некоторых случаях вероятностные алгоритмы могут быть «дерандомизированы», т.е. конвертированы в детерминированные алгоритмы. Один из общих методов, позволяющих сделать это, называется методом условных вероятностей.

\noindent Опишем этот подход на следующей задаче: имеется величина $X(x)$, где в булевом векторе $x=(x_{1} ,...,x_{n} )$ компоненты являются независимыми случайными величинами, причем $P\left\{x_{i} =1\right\}=p_{i} ,\quad P\left\{x_{i} =0\right\}=1-p_{i} $.

 Так в задаче о максимальной выполнимости КНФ $X(x_{1} ,...,x_{n} )$ равно числу невыполненных скобок в КНФ при вероятностном округлении.

 Требуется найти булев вектор $\hat{x}$, для которого выполнено неравенство
\[X(\hat{x})\le EX\] 

Обозначим через $X(x|x_{1} =d_{1} ,...,x_{k} =d_{k} )$ новую случайную величину, которая получена их $X$ фиксированием значений первых $k$ булевых переменных.

Рассмотрим покомпонентную стратегию определения искомого вектора $\hat{x}$. Для определения его первой компоненты вычисляем значения $f_{0} =EX(x|x_{1} =0)$ и $f_{1} =EX(x|x_{1} =1)$. Если $f_{0} <f_{1} $ полагаем $x_{1} =0$, иначе полагаем $x_{1} =1$. При определенной таким образом первой компоненты (обозначим ее $d_{1} $) вычисляем значение функционала $f_{0} =EX(x|x_{1} =d_{1} ,x_{2} =0)$ и $f_{1} =EX(x|x_{1} =d_{1} ,x_{2} =1)$. Если $f_{0} <f_{1} $ полагаем $x_{2} =0$, иначе полагаем $x_{2} =1$. Фиксируем вторую координату (обозначая ее $d_{2} $) и продолжаем описанный процесс до тех пор, пока не определится последняя компонента решения.

Найденный вектор будет удовлетворять требованию минимизации оценки математического ожидания. Рассмотрим первый шаг алгоритма. Имеем:
\[\begin{array}{l} {EX=P\left\{x_{1} =1\right\}EX(x|x_{1} =1)+P\left\{x_{1} =0\right\}EX(x|x_{1} =0)=} \\ {=p_{1} EX(x|x_{1} =1)+(1-p_{1} )EX(x|x_{1} =0)\ge } \\ {\ge p_{1} EX(x|x_{1} =d_{1} )+(1-p_{1} )EX(x|x_{1} =d_{1} )=} \\ {=EX(x|x_{1} =d_{1} )} \end{array}\] 
Продолжая эту цепочку неравенств для каждого шага алгоритма, получаем на $n$-ом шаге:
\[EX\ge EX(x|x_{1} =d_{1} ,...,x_{n} =d_{n} )\] 
Но $EX(x|x_{1} =d_{1} ,...,x_{n} =d_{n} )=X(x|x_{1} =d_{1} ,...,x_{n} =d_{n} )$.

Таким образом, изложенный общий метод позволяет осуществить «дерандомизацию», если есть эффективный алгоритм вычисления условных математических ожиданий (или условных вероятностей).

Применим метод условных вероятностей, к задаче максимальной выполнимости КНФ. Математическое ожидание числа невыполненных скобок равно:
\[\begin{array}{l} {EX=\sum _{j=1}^{m}P_{j}  ,} \\ {P_{j} =P\left\{\sum _{i\in C_{j}^{+} }x_{i} +\sum _{i\in C_{j}^{-} }(1-x_{i} )  =0\right\}} \end{array}\] 
Важный вопрос заключается в том, как эффективно вычислять условные математические ожидания. Предположим, что значения первых $k$ переменных уже определены и $I_{0} $ - множество индексов тех переменных, значения которых равно 0, а $I_{1} $ - множество индексов тех переменных, значения которых равно 1. Если $I_{0} \cap C_{j}^{-} \ne \emptyset $ или $I_{1} \cap C_{j}^{+} \ne \emptyset $, то $P_{j} ^{k} =P\left\{\sum _{i\in C_{j}^{+} }x_{i} +\sum _{i\in C_{j}^{-} }(1-x_{i} )  =0|x_{1} =d_{1} ,...,x_{k} =d_{k} \right\}=0$. В противном случае
\[P_{j} ^{k} =\prod _{i\in C_{j}^{+} \backslash I_{0} }(1-p_{i} ) \prod _{i\in C_{j}^{-} \backslash I_{1} }p_{i}  \] 

 Для полученного вектора$(d_{1} ,...,d_{n} )$ выполнено$X\left(x_{1} =d_{1} ,...,x_{n} =d_{n} \right)\le EX\le \frac{1}{e} m$

\noindent Таким образом, мы находим допустимый 0-1 вектор с гарантированной верхней оценкой для целевой функции.

\end{problem}


\begin{problem}

(Задача о покрытии). Дано конечное множество из m элементов и система его подмножеств $S_{1} ,...,S_{n} $. Требуется найти минимальную по числу подмножеств подсистему $S_{1} ,...,S_{n} $, покрывающую все множество объектов. 

\noindent Сформулируем ее в терминах булевых матриц и целочисленного линейного программирования:
\[\left\{\begin{array}{l} {cx\to \min ,} \\ {Ax\ge b,} \\ {\forall j\; x_{j} \in \{ 0,1\} .} \end{array}\right. \] 
Здесь переменные $x_{1} ,...,x_{n} $ соответствуют включению подмножеств $S_{1} ,...,S_{n} $ в решение-покрытие, матрица $A$ - матрица инцидентности, $c=(1...1)^{T} \in {\mathbb R}^{n} ,\quad b=(1...1)^{T} \in {\mathbb R}^{m} $ - векторы стоимости и ограничений.

\noindent \textit{Упражнение.} Что значат m строк ограничений? Чему равна целевая функция?  Что характеризует столбец матрицы инцидентности? (в терминах множеств в объектов)

\noindent Пусть элементы матрицы инцидентности -- независимые случайные величины с бернулевским распределением:$P\{ a_{ij} =1\} =p,$ $P\{ a_{ij} =0\} =1-p$. 

\noindent Для решения задачи применим жадный алгоритм. (Основная эвристика жадных алгоритмов -- на каждом шаге выбирать подмножество, максимально покрывающее еще не покрытые объекты.) 

\noindent \textbf{Теорема.} Пусть для случайной матрицы $A$, определенной выше, выполнены соотношения:
\[\begin{array}{l} {\forall \gamma >0:} \\ {\frac{\ln n}{m^{\gamma } } \mathop{\to }\limits_{n\to \infty } 0,} \\ {\frac{\ln m}{n} \mathop{\to }\limits_{n\to \infty } 0.} \end{array}\] 
Тогда для $\forall \varepsilon >0:$ $P\left\{\frac{Z}{M} \le 1+\varepsilon \right\}\mathop{\to }\limits_{n\to \infty } 1$, где Z -- решение жадного алгоритма, M -- величина минимального покрытия. 

\noindent \textit{Доказательство. }

\noindent Сначала покажите, что почти наверное величина минимального покрытия не меньше $l_{0} (\delta )=-\left\lceil (1-\delta )\frac{\ln m}{\ln (1-p)} \right\rceil $, т.е. $P\{ M\ge l_{0} \} \to 1$. Для этого введем случайную величину X, равную числу покрытий размера $l_{0} (\delta )$. И покажем, что $P\{ X\ge 1\} \to 0$. Для этого достаточно (из неравенства Чебышева-Маркова) показать, что $EX=C_{n}^{l_{o} } (1-(1-p)^{l_{0} } )^{m} \to 0$. 

\noindent Далее, покажите верхнюю оценку размера покрытия жадным алгоритмом на почти всех входах.

\noindent Для этого воспользуйтесь неравенством для больших уклонений:

\noindent $Y_{i} =\left\{\begin{array}{l} {1,\; p;} \\ {0,\; 1-p} \end{array}\right. $, тогда$P\left\{\left|\sum _{i=1}^{n}Y_{i}  -np\right|>\delta np\right\}\le 2\exp \left\{-\frac{\delta ^{2} }{3} np\right\}$.

\noindent Из этого утверждения покажите, что среднее число «плохих» столбцов (у которых число единиц менее, чем $(1-\delta )np$) стремится к нули с ростом n.

\noindent Пусть $N_{t} $ - число непокрытых строк после t-ого шага жадного алгоритма. Имеем:
\[N_{t} \le N_{t-1} -\frac{N_{t-1} (1-\delta )pn}{n} =N_{t-1} (1-(1-\delta )p)^{t} =m(1-(1-\delta )p)^{t} .\] 
Откуда максимальное t, при котором есть еще непокрытый элемент:
\[m(1-(1-\delta )p)^{t} \Rightarrow t\le -\frac{\ln m}{\ln (1-(1-\delta )p)} \] 
Откуда получается верхняя оценка мощности жадного покрытия в типичном случае:
\[P\left\{Z\le -\frac{\ln m}{\ln (1-(1-\delta )p)} \right\}\to 1\] 
Комбинируя, с нижней оценкой минимального покрытия, получим, что $\forall \varepsilon >0:$$P\left\{\frac{Z}{M} \le 1+\varepsilon \right\}\mathop{\to }\limits_{n\to \infty } 1$. Воспользуйтесь тем, что $\forall \varepsilon >0$ $\exists \delta :$
\[\frac{Z}{M} \le \frac{\ln (1-p)}{(1-\delta )\ln (1-p(1-\delta ))} \le (1-\delta )^{-1} +o(1)\le 1+\varepsilon .\] 

\end{problem}




\subsection{Кодирование}
\subsection{Теория игр}
\begin{problem}
Один игрок прячет (зажимает в кулаке) одну или две монеты достоинством 10 рублей. Другой игрок должен отгадать сколько денег у первого спрятано. Если отгадывает, то получает деньги, если нет -- платит 15 рублей. Каковы ``должны'' быть стратегии игроков при многократном повторении игры?

\end{problem}